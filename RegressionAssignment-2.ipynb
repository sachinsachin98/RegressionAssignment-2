{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b766399-a590-475c-934c-4d4517e7cc5e",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "R-squared, also known as the coefficient of determination, is a statistical measure used to evaluate the goodness of fit of a linear regression model. It quantifies the proportion of the variance in the dependent variable (the variable you are trying to predict) that is explained by the independent variables (predictors) in the model. In other words, R-squared indicates how well the independent variables account for the variation in the dependent variable.\n",
    "\n",
    "\n",
    "Calculation:\n",
    "R-squared is calculated as the square of the correlation coefficient (r) between the observed values of the dependent variable (Y) and the predicted values (Ŷ) from the linear regression model. Mathematically, it can be expressed as:\n",
    "\n",
    "R-squared (R²) = (1 - (SSR / SST))\n",
    "\n",
    "SSR (Sum of Squared Residuals) represents the sum of the squared differences between the observed values (Y) and the predicted values (Ŷ).\n",
    "SST (Total Sum of Squares) represents the sum of the squared differences between the observed values (Y) and the mean of the observed values (Ȳ). It essentially quantifies the total variance in the dependent variable.\n",
    "\n",
    "Interpretation:\n",
    "R-squared values range from 0 to 1, or 0% to 100%. The interpretation of R-squared is as follows:\n",
    "\n",
    "R-squared = 0: None of the variance in the dependent variable is explained by the independent variables. The model does not fit the data well.\n",
    "\n",
    "R-squared = 1: 100% of the variance in the dependent variable is explained by the independent variables. The model perfectly fits the data.\n",
    "\n",
    "R-squared between 0 and 1: The proportion of variance in the dependent variable explained by the independent variables. Higher R-squared values indicate a better fit, where a larger portion of the variance is explained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15baa3f5-87b0-488a-ad9e-63753c60ca43",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "Adjusted R-squared is a modified version of the regular R-squared (coefficient of determination) in linear regression. It takes into account the number of predictors (independent variables) in the model and provides a more conservative and informative measure of the goodness of fit. Adjusted R-squared addresses one of the limitations of the regular R-squared by penalizing the inclusion of unnecessary or irrelevant predictors in the model.\n",
    "\n",
    "Here's how adjusted R-squared differs from the regular R-squared:\n",
    "\n",
    "Calculation:\n",
    "Adjusted R-squared is calculated using the formula:\n",
    "\n",
    "Adjusted R-squared (R²_adj) = 1 - [(1 - R²) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "R² is the regular R-squared.\n",
    "n is the number of data points (samples).\n",
    "k is the number of independent variables (predictors) in the model.\n",
    "Penalizing Complexity:\n",
    "The key difference between adjusted R-squared and the regular R-squared is the inclusion of the term (n - 1) / (n - k - 1) in the formula. This term penalizes the model for including more independent variables, and it adjusts R-squared based on the number of predictors. As the number of predictors increases, the adjusted R-squared value will decrease unless the additional predictors significantly improve the model's fit. In other words, it discourages the inclusion of irrelevant variables that do not add explanatory power.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Like regular R-squared, adjusted R-squared values also range from 0 to 1.\n",
    "A higher adjusted R-squared indicates a better fit, considering the trade-off between explanatory power and model complexity.\n",
    "It provides a more conservative assessment of the model's goodness of fit because it accounts for the number of predictors and helps to guard against overfitting.\n",
    "Selecting Models:\n",
    "When comparing multiple regression models or deciding which variables to include in a model, adjusted R-squared is often more useful than the regular R-squared. It helps researchers and data analysts choose the most parsimonious model that explains the data well without unnecessarily adding complexity. Models with a higher adjusted R-squared are preferred, as long as the increase in R-squared justifies the inclusion of additional predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c6378-dad1-46fa-b56c-1c4afdf2621a",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "Adjusted R-squared is more appropriate to use in the following situations:\n",
    "\n",
    "Comparing Models with Different Numbers of Predictors: When you are comparing multiple regression models with varying numbers of independent variables, adjusted R-squared is a better choice. It helps you assess which model provides a better balance between explanatory power and model simplicity. Models with higher adjusted R-squared values are preferred, as long as the increase in R-squared justifies the inclusion of additional predictors.\n",
    "\n",
    "Model Selection and Feature Engineering: Adjusted R-squared is valuable when you are selecting the most appropriate predictors (features) for your model. It helps you identify which variables are contributing meaningfully to the explanation of the dependent variable and which ones are not. This is especially useful when you want to avoid overfitting by excluding irrelevant variables.\n",
    "\n",
    "Guarding Against Overfitting: In cases where you want to build a model that generalizes well to new, unseen data, adjusted R-squared is a crucial tool. It discourages the inclusion of too many predictors that may capture noise in the training data but do not generalize to new data.\n",
    "\n",
    "Complex Models: When dealing with complex models, such as high-dimensional regression or when you have a large number of predictors, adjusted R-squared is especially important. It helps you evaluate model performance while accounting for the added complexity brought about by the many predictors.\n",
    "\n",
    "Research and Interpretability: If you are conducting research and need to convey the strength and relevance of the predictors to a non-technical audience, adjusted R-squared provides a more accurate assessment of model fit by considering model complexity. This can be particularly important when explaining the significance of the predictors and the overall model.\n",
    "\n",
    "Quality Control: In situations where data quality is a concern, adjusted R-squared can be a helpful metric. It encourages you to scrutinize whether including more variables improves the model's performance or if it just adds noise from data measurement errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88570b-3689-4221-ba95-e958a7f3a5b9",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are common evaluation metrics used to assess the accuracy and performance of a predictive model. These metrics provide measures of how well the model's predictions align with the actual observed values of the dependent variable.\n",
    "\n",
    "Here's an explanation of each of these metrics, how they are calculated, and what they represent:\n",
    "\n",
    "MSE (Mean Squared Error):\n",
    "\n",
    "Calculation: MSE is computed by taking the average of the squared differences between the predicted values and the actual values for each data point. Mathematically, it is expressed as:\n",
    "\n",
    "MSE = (1/n) * Σ(yi - ŷi)², where i ranges from 1 to n (number of data points).\n",
    "\n",
    "Interpretation: MSE quantifies the average squared difference between the predicted and actual values. It measures the average \"loss\" or error of the model's predictions. A lower MSE indicates a better fit, with smaller errors. However, it tends to emphasize larger errors due to the squaring of differences.\n",
    "\n",
    "RMSE (Root Mean Squared Error):\n",
    "\n",
    "Calculation: RMSE is the square root of the MSE and is calculated as follows:\n",
    "\n",
    "RMSE = √(MSE)\n",
    "\n",
    "Interpretation: RMSE provides a measure of the average magnitude of the errors in the same units as the dependent variable. It is often considered more interpretable than MSE because it provides a metric in the same scale as the original data. Lower RMSE values indicate better model performance.\n",
    "\n",
    "MAE (Mean Absolute Error):\n",
    "\n",
    "Calculation: MAE is computed by taking the average of the absolute differences between the predicted and actual values for each data point. Mathematically, it is expressed as:\n",
    "\n",
    "MAE = (1/n) * Σ|yi - ŷi|, where i ranges from 1 to n (number of data points).\n",
    "\n",
    "Interpretation: MAE represents the average absolute difference between the predicted and actual values. Unlike MSE and RMSE, MAE does not emphasize larger errors because it uses the absolute differences. It provides a robust measure of error that is less sensitive to outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4a80b-641e-4ac5-8304-e3d8acf2edb8",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "\n",
    "Using RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) as evaluation metrics in regression analysis comes with both advantages and disadvantages. Let's explore these for each metric:\n",
    "\n",
    "Advantages of RMSE:\n",
    "\n",
    "Sensitivity to Large Errors: RMSE gives more weight to larger errors due to the squaring of differences. This can be advantageous when large errors are of particular concern because it punishes models more for significant deviations from the actual values.\n",
    "\n",
    "Consistency with Modeling Objectives: In some cases, RMSE may align better with the specific modeling objectives or loss functions, making it a preferred choice.\n",
    "\n",
    "Disadvantages of RMSE:\n",
    "\n",
    "Sensitivity to Outliers: RMSE is highly sensitive to outliers, which means that a single extreme value can significantly inflate the RMSE. This may not be suitable for datasets with noisy or erroneous data points.\n",
    "\n",
    "Interpretability: RMSE is not as intuitive to interpret as MAE because it is in the squared units of the dependent variable, which may not be as meaningful.\n",
    "\n",
    "Advantages of MSE:\n",
    "\n",
    "Mathematical Properties: MSE is mathematically convenient and has several statistical properties that make it useful in statistical analysis.\n",
    "Disadvantages of MSE:\n",
    "\n",
    "Sensitivity to Outliers: Similar to RMSE, MSE is sensitive to outliers. A single outlier can have a disproportionate impact on the MSE.\n",
    "\n",
    "Lack of Interpretability: Like RMSE, MSE is not particularly intuitive to interpret because it is in squared units, which may not have direct real-world meaning.\n",
    "\n",
    "Advantages of MAE:\n",
    "\n",
    "Robustness: MAE is less sensitive to outliers compared to RMSE and MSE. It provides a more balanced view of the model's performance and is more robust in the presence of extreme values.\n",
    "\n",
    "Interpretability: MAE is highly interpretable as it represents the average absolute error in the same units as the dependent variable. It's easier to explain to non-technical audiences.\n",
    "\n",
    "Disadvantages of MAE:\n",
    "\n",
    "Neglect of Larger Errors: MAE does not emphasize larger errors as much as RMSE and MSE do. If larger errors are of particular concern, MAE may not be the best choice.\n",
    "\n",
    "Statistical Properties: MAE does not possess some of the desirable mathematical properties that MSE and RMSE have, which can be important in certain statistical analyses.\n",
    "\n",
    "In practice, the choice of which metric to use depends on the specific context of the problem and the goals of the analysis:\n",
    "\n",
    "RMSE or MSE may be preferred when you want to give more importance to larger errors or when the modeling objectives align with these metrics.\n",
    "MAE is a robust choice when you want a more balanced view of model performance and interpretability is a priority, especially in cases where outliers can distort the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ee41e-30ff-4ff8-a656-da6e2597deca",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression and other linear models to prevent overfitting and select a subset of the most relevant predictors by adding a penalty term to the cost function. Lasso differs from Ridge regularization in the type of penalty applied and the way it impacts the coefficients of the independent variables.\n",
    "\n",
    "Here's an explanation of Lasso regularization and how it differs from Ridge regularization:\n",
    "\n",
    "Lasso Regularization:\n",
    "\n",
    "Penalty Term: Lasso adds a penalty term to the linear regression's cost function that is equal to the absolute values of the coefficients (L1 regularization). Mathematically, it is expressed as:\n",
    "\n",
    "Cost function = Least Squares Error (MSE) + λ * Σ|βi|\n",
    "\n",
    "The Σ|βi| term encourages the absolute values of the coefficients to be as small as possible, effectively pushing some of them to become exactly zero.\n",
    "Sparsity: One of the significant characteristics of Lasso regularization is that it encourages sparsity, meaning that it can set some coefficients to zero. In other words, Lasso can perform both feature selection and model regularization simultaneously, identifying and discarding irrelevant predictors.\n",
    "\n",
    "Differences from Ridge Regularization:\n",
    "\n",
    "Penalty Type: The primary difference between Lasso and Ridge regularization is the type of penalty applied to the coefficients. Ridge uses the square of the coefficients (L2 regularization), while Lasso uses the absolute values of the coefficients (L1 regularization).\n",
    "\n",
    "Coefficient Shrinking: Ridge regularization primarily shrinks the coefficients toward zero but does not force them to become exactly zero. In contrast, Lasso can set coefficients to precisely zero, effectively removing those predictors from the model. This makes Lasso a more aggressive feature selection method compared to Ridge.\n",
    "\n",
    "When to Use Lasso Regularization:\n",
    "\n",
    "Lasso regularization is more appropriate to use when:\n",
    "\n",
    "You suspect that many of the predictors in your model may be irrelevant or redundant. Lasso can automatically select the most important features and discard the rest.\n",
    "\n",
    "You want a simpler and more interpretable model. Lasso can help create a sparse model with fewer predictors, making it easier to understand and explain.\n",
    "\n",
    "You are dealing with high-dimensional data where the number of predictors is significantly greater than the number of data points. Lasso can be particularly effective in feature selection in such cases.\n",
    "\n",
    "You want to avoid multicollinearity (correlation between predictors). Lasso can handle multicollinearity by selecting one predictor from a group of correlated variables while setting the others to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487ac17-d3d4-4d59-9ec0-048057c7c2fa",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "Regularized linear models help prevent overfitting in machine learning by introducing a penalty term into the model's cost function that discourages the coefficients of the independent variables from taking extremely large values. This penalty term effectively reduces the model's complexity, making it less prone to overfitting. Here's how it works, using Ridge and Lasso regularization as examples:\n",
    "\n",
    "Ridge Regularization:\n",
    "\n",
    "In Ridge regularization, a L2 penalty term is added to the linear regression's cost function. The cost function for Ridge is as follows:\n",
    "\n",
    "Cost function = Least Squares Error (MSE) + λ * Σ(βi²)\n",
    "\n",
    "The Σ(βi²) term encourages the squared values of the coefficients to be as small as possible.\n",
    "λ is the regularization strength, and it determines the trade-off between fitting the data well and keeping the coefficients small.\n",
    "The Ridge regularization adds a constraint that limits the magnitude of the coefficients. As a result, Ridge regression tends to shrink the coefficients towards zero without setting them exactly to zero. This reduces the risk of overfitting because the model is less able to fit noise in the training data by inflating the magnitudes of the coefficients.\n",
    "\n",
    "Lasso Regularization:\n",
    "\n",
    "In Lasso regularization, an L1 penalty term is added to the cost function. The cost function for Lasso is as follows:\n",
    "\n",
    "Cost function = Least Squares Error (MSE) + λ * Σ|βi|\n",
    "\n",
    "The Σ|βi| term encourages the absolute values of the coefficients to be as small as possible.\n",
    "λ is the regularization strength, controlling the trade-off between fitting the data well and keeping the coefficients small.\n",
    "Lasso regularization not only shrinks the coefficients but can also set some of them exactly to zero. This property makes Lasso suitable for feature selection. It encourages sparsity by identifying and discarding irrelevant predictors, which can significantly reduce the model's complexity and prevent overfitting.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a simple linear regression problem where you have a dataset of housing prices based on various features such as square footage, number of bedrooms, and location. You want to build a predictive model to estimate house prices. If you use a regular linear regression model, it may fit the training data very closely, capturing noise and idiosyncrasies specific to the training dataset.\n",
    "\n",
    "Now, let's introduce Ridge and Lasso regularization:\n",
    "\n",
    "If you apply Ridge regularization, the model will constrain the coefficients, making them smaller. This reduces the chances of the model fitting random fluctuations in the data, thus preventing overfitting.\n",
    "\n",
    "If you apply Lasso regularization, it can not only shrink the coefficients but also set some of them to exactly zero. This means the model may exclude some irrelevant features entirely, further reducing the risk of overfitting.\n",
    "\n",
    "In both cases, the regularization techniques help create simpler and more robust models that generalize better to new, unseen data, making them effective tools for preventing overfitting in machine learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250480a4-2c48-494a-90c0-551af625c7d4",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "Regularized linear models, such as Ridge and Lasso regression, offer valuable tools for regression analysis, but they also have limitations and may not always be the best choice for every scenario. Here are some limitations and situations where regularized linear models may not be the optimal choice:\n",
    "\n",
    "Loss of Predictive Power: Regularized linear models are primarily designed for preventing overfitting by shrinking coefficients. In doing so, they may sacrifice some predictive power. If the relationships between independent variables and the dependent variable are truly linear and non-noisy, standard linear regression without regularization might provide a better fit.\n",
    "\n",
    "Lack of Interpretability: Regularized models tend to result in shrunken and sometimes near-zero coefficients. While this can reduce overfitting, it also makes the model less interpretable. In some cases, particularly when interpretability is crucial, a simple linear model might be more suitable.\n",
    "\n",
    "Hyperparameter Tuning: Regularized models require tuning of the hyperparameter (λ or alpha) to balance between model complexity and fit to the data. Selecting the right hyperparameter can be challenging and may involve cross-validation. This process can be computationally intensive and may not always lead to the best model if not done carefully.\n",
    "\n",
    "Inadequate Handling of Non-Linear Relationships: Regularized linear models are inherently linear. If your data exhibits complex, non-linear relationships, other methods like decision trees, random forests, or support vector machines might perform better without the need for regularization.\n",
    "\n",
    "Sensitivity to Outliers: Ridge and Lasso regularization are still sensitive to extreme outliers, although to a lesser extent than non-regularized linear regression. In cases where outliers are a significant concern, robust regression techniques might be more appropriate.\n",
    "\n",
    "High Feature-to-Data Ratio: If you have an exceptionally high number of features relative to the amount of data (a high feature-to-data ratio), regularization may be necessary, but it can also be challenging to implement effectively. You'll need to consider other dimensionality reduction techniques or data preprocessing methods.\n",
    "\n",
    "Handling Categorical Data: Regularized linear models are less suitable for datasets with categorical variables, particularly those with high cardinality (many unique categories). Encoding categorical data appropriately can be complex in the context of these models.\n",
    "\n",
    "Context-Specific Needs: The choice of the right model depends on the specific context, the goals of the analysis, and the nature of the data. In some cases, simpler models may work just as well without the complexity of regularization.\n",
    "\n",
    "Robustness to Model Assumptions: Regularized linear models still assume a linear relationship between the independent variables and the dependent variable. If this assumption does not hold in your data, a different modeling approach might be more suitable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d543c86-1f3e-41f7-b0c3-be262571ebf9",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "Choosing the better-performing model between Model A with an RMSE of 10 and Model B with an MAE of 8 depends on your specific goals and the characteristics of your problem. Let's consider the differences between these metrics and the limitations of the choice:\n",
    "\n",
    "Model A (RMSE = 10):\n",
    "\n",
    "RMSE (Root Mean Squared Error) gives more weight to larger errors because it squares the differences.\n",
    "It tends to penalize the model for larger errors more strongly.\n",
    "RMSE is more sensitive to outliers than MAE.\n",
    "Model B (MAE = 8):\n",
    "\n",
    "MAE (Mean Absolute Error) does not emphasize larger errors and treats all errors equally.\n",
    "It provides a measure of the average absolute error in the same units as the dependent variable.\n",
    "MAE is less sensitive to outliers compared to RMSE.\n",
    "Your choice between Model A and Model B depends on your specific context and goals:\n",
    "\n",
    "If you are concerned about larger errors: If you want a model that minimizes the impact of larger errors and prioritizes reducing the largest discrepancies between predicted and actual values, then you might prefer Model A with the lower RMSE.\n",
    "\n",
    "If you want a more balanced view: If you want a model that provides a more balanced view of prediction accuracy and is less influenced by outliers, you might prefer Model B with the lower MAE. MAE is often considered more robust and interpretable.\n",
    "\n",
    "Interpretability: MAE is generally more interpretable because it represents the average absolute error in the same units as the dependent variable. This could be an advantage if you need to explain the model's performance to non-technical stakeholders.\n",
    "\n",
    "Context and Specific Problem: The choice of metric also depends on the specific problem you are solving. Some problems may prioritize accuracy in predicting small errors, while others may require minimizing the impact of large errors.\n",
    "\n",
    "Outliers: Consider the presence of outliers in your dataset. RMSE is more sensitive to outliers, so if outliers are a concern, MAE might be a more appropriate choice.\n",
    "\n",
    "It's important to be aware of the limitations of both metrics:\n",
    "\n",
    "Both RMSE and MAE only provide information about the magnitude of errors but not the direction (overestimation or underestimation).\n",
    "The choice of one metric over the other does not necessarily make one model universally better than the other. You should consider the specific context and requirements of your problem.\n",
    "Consider using other metrics or a combination of metrics, along with domain knowledge, to make a more comprehensive assessment of model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85361072-2dd7-4667-b2ad-02315166f96a",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "The choice between Ridge and Lasso regularization, represented by Model A and Model B, respectively, depends on the specific context and goals of your analysis. Let's compare the two regularization methods and consider the trade-offs and limitations:\n",
    "\n",
    "Model A (Ridge Regularization, λ = 0.1):\n",
    "\n",
    "Ridge regularization adds an L2 penalty to the linear regression's cost function, encouraging the coefficients to be small, but not necessarily zero.\n",
    "It is effective at reducing multicollinearity (correlation between predictors) and can be used to prevent overfitting.\n",
    "A smaller value of λ (0.1 in this case) means a weaker penalty and allows the model to retain more of the original features.\n",
    "Model B (Lasso Regularization, λ = 0.5):\n",
    "\n",
    "Lasso regularization adds an L1 penalty to the cost function, encouraging both coefficient shrinkage and feature selection. It can set some coefficients to exactly zero.\n",
    "It is useful for feature selection and model simplification. With a stronger λ (0.5 in this case), Lasso is more likely to eliminate irrelevant predictors.\n",
    "Your choice between Model A (Ridge) and Model B (Lasso) depends on your specific objectives:\n",
    "\n",
    "Ridge Regularization (Model A):\n",
    "\n",
    "Choose Ridge if you believe that most of your predictors are relevant and you want to reduce the risk of multicollinearity.\n",
    "Ridge tends to be less aggressive in feature selection compared to Lasso.\n",
    "It retains most of the predictors but shrinks their coefficients to prevent overfitting.\n",
    "Lasso Regularization (Model B):\n",
    "\n",
    "Choose Lasso if you suspect that many of your predictors are irrelevant and you want a simpler, more interpretable model.\n",
    "Lasso can set some coefficients to zero, effectively eliminating the corresponding predictors.\n",
    "Lasso can help with feature selection and create a sparse model.\n",
    "Trade-offs and Limitations:\n",
    "\n",
    "Loss of Predictive Power: Both Ridge and Lasso regularization introduce a bias that may lead to a loss of predictive power compared to non-regularized linear regression when the relationships between predictors and the dependent variable are truly linear and non-noisy.\n",
    "\n",
    "Choice of λ (Regularization Strength): The choice of the regularization strength (λ) for Ridge and Lasso can significantly impact the results. Selecting the appropriate λ requires cross-validation or other methods, and the optimal value may vary between datasets.\n",
    "\n",
    "Complexity of Interpretation: As the regularization strength increases, the interpretability of the model decreases. Lasso, in particular, can lead to sparse models that may be more challenging to explain.\n",
    "\n",
    "Handling Categorical Variables: Regularization methods like Ridge and Lasso may not handle categorical variables well, especially those with high cardinality. Preprocessing of categorical data is needed.\n",
    "\n",
    "Linearity Assumption: Both Ridge and Lasso assume linearity between the predictors and the dependent variable. If this assumption is not met, other modeling techniques may be more suitable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1104c-bb1b-4868-a3f5-46f3707ba614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
